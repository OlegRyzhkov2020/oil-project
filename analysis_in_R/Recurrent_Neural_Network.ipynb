{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "from numpy import concatenate\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 8 #Number of features in the dataset\n",
    "lag_steps = 1 #Number of lagged time features to be generated\n",
    "label_feature = ‘WTI’ #The column in dataset that model is being built to predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function arranges the dataset to be used for surpervised learning by shifting the input values of features by the number\n",
    "# time steps given in lag_steps\n",
    "\n",
    "def sequential_to_supervised(data, lag_steps = 1, n_out = 1, dropnan = True):\n",
    "    features = 1 if type(data) is list else data.shape[1] # Get the number of features in dataset\n",
    "    df = DataFrame(data)\n",
    "    cols = list()\n",
    "    feature_names = list()\n",
    "    \n",
    "    for i in range(lag_steps, 0, -1):\n",
    "        cols.append(df.shift(i)) # This will be the shifted dataset\n",
    "        feature_names += [(str(df.columns[j])) + '(t-%d)' % (i) for j in range(features)] # Names of the shifted features\n",
    "    \n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            feature_names += [(str(df.columns[j])) + '(t)'  for j in range(features)] # Names of the shifted features\n",
    "        else:\n",
    "            feature_names += [(str(df.columns[j])) + '(t+%d)' % (i) for j in range(features)] # Names of the shifted features\n",
    "    \n",
    "    agg = concat(cols, axis=1) \n",
    "    agg.columns = feature_names\n",
    "    \n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the dataset which is in .csv format, has column headings and has an index column\n",
    "dataset = read_csv(\"Dataset.csv\", header = 0, index_col = 0, squeeze = True, usecols = (i for i in range(0, num_features+1)))\n",
    "supervised_dataset = sequential_to_supervised(dataset, lag_steps)\n",
    "\n",
    "# Move label column to the end of dataset\n",
    "cols_at_end = [label_feature + '(t)']\n",
    "supervised_dataset = supervised_dataset[[c for c in supervised_dataset if c not in cols_at_end] + [c for c in cols_at_end if c in supervised_dataset]]\n",
    "\n",
    "#print(supervised_dataset.shape)  # Used for debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the current timestep columns of features other than the one being predicted, which will be the label or y \n",
    "supervised_dataset.drop(supervised_dataset.columns[(num_features*lag_steps) : (num_features*lag_steps + num_features -1)], axis=1, inplace=True)\n",
    "#print(supervised_dataset.shape) # Used for debugging\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "supervised_dataset_scaled = scaler.fit_transform(supervised_dataset) # Scaling all values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(supervised_dataset_scaled.shape[0]*0.8) # Splitting for traning and testing\n",
    "train = supervised_dataset_scaled[:split, :]\n",
    "test = supervised_dataset_scaled[split:, :]\n",
    "\n",
    "train_X, train_y = train[:, :-1], train[:, -1] # The label column is separated out\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "#print(train_X.shape) # Used for debugging\n",
    "#print(test_X.shape) # Used for debugging\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1])) # Reshaping done for LSTM as it need 3D input\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "#print(train_X.shape) # Used for debugging\n",
    "#print(test_X.shape) # Used for debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the LSTM model to be fit\n",
    "model = Sequential()\n",
    "model.add(LSTM(85, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "# Fitting the model\n",
    "history = model.fit(train_X, train_y, epochs=70, batch_size=175, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "# Plotting the training progression\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p36workshop",
   "language": "python",
   "name": "p36workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
